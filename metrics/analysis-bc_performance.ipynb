{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import libraries and metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict as dd\n",
    "\n",
    "RESULTS_PATH = \"results/16c_1000ops_3/\"\n",
    "CLIENTS_METRICS = \"clients.json\"\n",
    "NODES_METRICS = \"node%d.json\"\n",
    "\n",
    "PLOTS_OUT_PATH = \"plots/\"\n",
    "\n",
    "with open(RESULTS_PATH + CLIENTS_METRICS, \"r\") as f:\n",
    "    clients_metrics = list(map(lambda x: json.loads(x), f.readlines()))\n",
    "\n",
    "nodes_metrics = {}\n",
    "for i in range(1, 5):\n",
    "    with open(RESULTS_PATH + NODES_METRICS % i, \"r\") as f:\n",
    "        nodes_metrics[i] = list(map(lambda x: json.loads(x), f.readlines()))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Correctness Tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correct Blockchain Order"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if 2f+1 nodes have the same number of blocks\n",
    "num_blocks = {}\n",
    "for i in range(1, 5):\n",
    "    num_blocks[i] = len(list(filter(lambda x: x[\"metric\"] == \"committed_block\", nodes_metrics[i])))\n",
    "if len(set(num_blocks.values())) > 1:\n",
    "    print(\"Different number of blocks in nodes\")\n",
    "    print(num_blocks)\n",
    "else:\n",
    "    print(\"All nodes have the same number of blocks: \" + str(num_blocks[1]))\n",
    "\n",
    "# Check if all nodes have the same blocks in same order\n",
    "blocks = dd(dict)\n",
    "for i in range(1, 5):\n",
    "    for block in list(filter(lambda x: x[\"metric\"] == \"committed_block\", nodes_metrics[i])):\n",
    "        blocks[i][int(block[\"seq\"])] = block[\"hash\"]\n",
    "\n",
    "for i in range(1, 5):\n",
    "    for j in range(1, 5):\n",
    "        for k in range(1, np.max(list(num_blocks.values())) + 1):\n",
    "            try:\n",
    "                if blocks[i][k] != blocks[j][k] and blocks[i][k]:\n",
    "                    print(\"ERROR: Different blocks in nodes in seq=\" + str(k))\n",
    "                    print(\"Node \" + str(i) + \": \" + blocks[i][k])\n",
    "                    print(\"Node \" + str(j) + \": \" + blocks[j][k])\n",
    "                    pass\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "print(\"All nodes have the same blocks in same order\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Statistics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Average Block Size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_block_size = np.mean(list(map(lambda x: int(x[\"num_ops\"]), filter(lambda x: x[\"metric\"] == \"committed_block\", nodes_metrics[1]))))\n",
    "print(\"Average block size: \" + str(avg_block_size) + \" operations\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Average immediate reply time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_reply = np.mean(list(map(lambda x: int(x[\"latency\"]), filter(lambda x: x[\"metric\"] == \"operation_reply\", clients_metrics))))\n",
    "print(\"Average reply time: \" + str(avg_reply) + \" ms\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Average operation execution time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_exec = np.mean(list(map(lambda x: int(x[\"latency\"]), filter(lambda x: x[\"metric\"] == \"operation_executed\", clients_metrics))))\n",
    "print(\"Average execution time: \" + str(avg_exec) + \" ms\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Executed Operations Throughput"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate start and end\n",
    "elapsed = int(clients_metrics[-1][\"time\"]) - int(clients_metrics[0][\"time\"])\n",
    "\n",
    "# calculate number of blocks\n",
    "num_blocks = len(list(filter(lambda x: x[\"metric\"] == \"operation_executed\", clients_metrics)))\n",
    "\n",
    "# calculate throughput\n",
    "throughput = num_blocks / (elapsed / 1000)\n",
    "\n",
    "print(\"Throughput: \" + str(throughput) + \" ops/s\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Block Throughput"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate start and end\n",
    "elapsed = int(nodes_metrics[1][-1][\"time\"]) - int(nodes_metrics[1][0][\"time\"])\n",
    "\n",
    "# calculate number of blocks\n",
    "num_blocks = len(list(filter(lambda x: x[\"metric\"] == \"committed_block\", nodes_metrics[1])))\n",
    "\n",
    "# calculate throughput\n",
    "throughput = num_blocks / (elapsed / 1000)\n",
    "\n",
    "print(\"Throughput: \" + str(throughput) + \" blocks/s\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Latency Per Operation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "RESULTS_PATH = \"results/\"\n",
    "CLIENTS_METRICS = \"clients.json\"\n",
    "\n",
    "PLOTS_OUT_PATH = \"plots/\"\n",
    "\n",
    "test = \"fr3\"\n",
    "\n",
    "latencies_executed_runs = []\n",
    "avg_latencies_runs = []\n",
    "\n",
    "# list of latencies for each run, average them all and then average each 100 ops\n",
    "for run in os.listdir(RESULTS_PATH):\n",
    "    if test not in run:\n",
    "        continue\n",
    "    with open(RESULTS_PATH + run + \"/\" + CLIENTS_METRICS, \"r\") as f:\n",
    "        latencies_executed_runs.append(list(map(lambda x: int(x[\"latency\"]), filter(lambda x: x[\"metric\"] == \"operation_executed\", list(map(lambda x: json.loads(x), f.readlines()))))))\n",
    "\n",
    "# for each operation in a run append average to avg_latencies_runs\n",
    "# mean for each operation\n",
    "max_len = np.max(list(map(lambda x: len(x), latencies_executed_runs)))\n",
    "print(max_len)\n",
    "for i in range(0, max_len):\n",
    "    avg_latencies_runs.append(np.mean(list(map(lambda x: x[i], filter(lambda l: len(l) > i, latencies_executed_runs)))))\n",
    "\n",
    "# mean for each 100 operations\n",
    "avg_latencies_runs = list(map(lambda x: np.max(x), np.array_split(avg_latencies_runs, len(avg_latencies_runs)/10)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(avg_latencies_runs)\n",
    "plt.ylabel(\"Latency (ms)\")\n",
    "plt.xlabel(\"Tens of Executed Operations\")\n",
    "plt.savefig(PLOTS_OUT_PATH + f\"latency_per_operation_{test}.pdf\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Throughput-Latency"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def throughput_run(clients_metrics_run, metric):\n",
    "    elapsed = int(clients_metrics_run[-1][\"time\"]) - int(clients_metrics_run[0][\"time\"])\n",
    "    num_ops = len(list(filter(lambda x: x[\"metric\"] == metric, clients_metrics_run)))\n",
    "    return num_ops / (elapsed / 1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def avg_latency_run(clients_metrics_run, metric):\n",
    "    return np.mean(list(map(lambda x: int(x[\"latency\"]), filter(lambda x: x[\"metric\"] == metric, clients_metrics_run))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def block_throughput_run(node_metrics_run):\n",
    "    elapsed = int(node_metrics_run[-1][\"time\"]) - int(node_metrics_run[0][\"time\"])\n",
    "    num_blocks = len(list(filter(lambda x: x[\"metric\"] == \"committed_block\", node_metrics_run)))\n",
    "    return num_blocks / (elapsed / 1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict as dd\n",
    "\n",
    "RESULTS_PATH = \"results/\"\n",
    "CLIENTS_METRICS = \"clients.json\"\n",
    "NODES_METRICS = \"node%d.json\"\n",
    "\n",
    "PLOTS_OUT_PATH = \"plots/\"\n",
    "\n",
    "wanted_id = 50\n",
    "\n",
    "throughputs_executed = dd(list)\n",
    "latencies_executed = dd(list)\n",
    "\n",
    "throughputs_reply = dd(list)\n",
    "latencies_reply = dd(list)\n",
    "\n",
    "# run format : 16c_{ops_sec}ops_{run_number}\n",
    "# group by ops_sec and run_number\n",
    "for run in os.listdir(RESULTS_PATH):\n",
    "    run_path = RESULTS_PATH + run + \"/\"\n",
    "    # get id with regex\n",
    "    id = int(re.search(r\"(\\d+)c\", run).group(1))\n",
    "    if id != wanted_id:\n",
    "        continue\n",
    "    # get ops_sec wit regex\n",
    "    ops_sec = int(re.search(r\"(\\d+)ops\", run).group(1))\n",
    "\n",
    "    with open(run_path + CLIENTS_METRICS, \"r\") as clients_file:\n",
    "        clients_metrics = list(map(lambda x: json.loads(x), clients_file.readlines()))\n",
    "        throughputs_executed[ops_sec].append(throughput_run(clients_metrics, \"operation_executed\"))\n",
    "        latencies_executed[ops_sec].append(avg_latency_run(clients_metrics, \"operation_executed\"))\n",
    "\n",
    "        throughputs_reply[ops_sec].append(throughput_run(clients_metrics, \"operation_reply\"))\n",
    "        latencies_reply[ops_sec].append(avg_latency_run(clients_metrics, \"operation_reply\"))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#ops_sec_list = [5, 10, 20, 40, 80, 160, 320, 480, 640, 700, 860, 1000]\n",
    "ops_sec_list = [5, 10, 20, 40, 80, 160, 320, 640, 1000]\n",
    "\n",
    "# plot throughput-latency (each throughput and latency is one point in line plot)\n",
    "# 1 line for replied and 1 line for executed\n",
    "plt.plot(list(map(lambda x: np.mean(throughputs_executed[x])/1000, ops_sec_list)), list(map(lambda x: np.mean(latencies_executed[x])/1000, ops_sec_list)), label=\"Writes\", marker='o', linestyle='-')\n",
    "plt.plot(list(map(lambda x: np.mean(throughputs_reply[x])/1000, ops_sec_list)), list(map(lambda x: np.mean(latencies_reply[x])/1000, ops_sec_list)), label=\"Reads & ACKs\", marker='o', linestyle='-')\n",
    "plt.xlabel(\"Throughput (1000 ops/s)\")\n",
    "plt.ylabel(\"Latency (s)\")\n",
    "plt.legend()\n",
    "plt.savefig(PLOTS_OUT_PATH + \"throughput_latency.pdf\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
